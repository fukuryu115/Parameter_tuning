{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40bd547-082b-4dd8-b13f-1ae8a550a517",
   "metadata": {},
   "source": [
    "# MNISTの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934f0314-1734-4194-b107-ea4888bb0497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 14:03:34.153179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 14:03:34.623135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 1 epoch. 100 iteration. Loss: 2.171548843383789\n",
      "Training: 1 epoch. 200 iteration. Loss: 2.2879350185394287\n",
      "Training: 1 epoch. 300 iteration. Loss: 1.1362601518630981\n",
      "Training: 1 epoch. 400 iteration. Loss: 1.970834493637085\n",
      "Training: 1 epoch. 500 iteration. Loss: 1.399919033050537\n",
      "Training: 1 epoch. 600 iteration. Loss: 1.8379899263381958\n",
      "Training: 1 epoch. 700 iteration. Loss: 1.3880672454833984\n",
      "Training: 1 epoch. 800 iteration. Loss: 1.5775820016860962\n",
      "Training: 1 epoch. 900 iteration. Loss: 2.076324939727783\n",
      "Training: 1 epoch. 1000 iteration. Loss: 1.879427433013916\n",
      "Training: 1 epoch. 1100 iteration. Loss: 2.5314388275146484\n",
      "Training: 1 epoch. 1200 iteration. Loss: 1.6357733011245728\n",
      "Training: 1 epoch. 1300 iteration. Loss: 1.9705355167388916\n",
      "Training: 1 epoch. 1400 iteration. Loss: 1.7092700004577637\n",
      "Training: 1 epoch. 1500 iteration. Loss: 2.02944016456604\n",
      "Training: 1 epoch. 1600 iteration. Loss: 1.5576140880584717\n",
      "Training: 1 epoch. 1700 iteration. Loss: 1.1855688095092773\n",
      "Training: 1 epoch. 1800 iteration. Loss: 2.187204599380493\n",
      "Training: 1 epoch. 1900 iteration. Loss: 2.0581138134002686\n",
      "Training: 1 epoch. 2000 iteration. Loss: 1.5476038455963135\n",
      "Training: 1 epoch. 2100 iteration. Loss: 1.7032949924468994\n",
      "Training: 1 epoch. 2200 iteration. Loss: 2.452005624771118\n",
      "Training: 1 epoch. 2300 iteration. Loss: 1.184840440750122\n",
      "Training: 1 epoch. 2400 iteration. Loss: 1.1711556911468506\n",
      "Training: 1 epoch. 2500 iteration. Loss: 1.887618899345398\n",
      "Training: 1 epoch. 2600 iteration. Loss: 1.334928274154663\n",
      "Training: 1 epoch. 2700 iteration. Loss: 1.3434628248214722\n",
      "Training: 1 epoch. 2800 iteration. Loss: 1.8519285917282104\n",
      "Training: 1 epoch. 2900 iteration. Loss: 1.2448737621307373\n",
      "Training: 1 epoch. 3000 iteration. Loss: 2.343465566635132\n",
      "Training: 1 epoch. 3100 iteration. Loss: 0.964951753616333\n",
      "Training: 1 epoch. 3200 iteration. Loss: 1.4586471319198608\n",
      "Training: 1 epoch. 3300 iteration. Loss: 1.311998724937439\n",
      "Training: 1 epoch. 3400 iteration. Loss: 1.2874739170074463\n",
      "Training: 1 epoch. 3500 iteration. Loss: 1.1711504459381104\n",
      "Training: 1 epoch. 3600 iteration. Loss: 1.3820650577545166\n",
      "Training: 1 epoch. 3700 iteration. Loss: 1.6686714887619019\n",
      "Training: 1 epoch. 3800 iteration. Loss: 2.1018784046173096\n",
      "Training: 1 epoch. 3900 iteration. Loss: 2.8199610710144043\n",
      "Training: 1 epoch. 4000 iteration. Loss: 1.139427661895752\n",
      "Training: 1 epoch. 4100 iteration. Loss: 1.4956080913543701\n",
      "Training: 1 epoch. 4200 iteration. Loss: 1.6588904857635498\n",
      "Training: 1 epoch. 4300 iteration. Loss: 1.2810614109039307\n",
      "Training: 1 epoch. 4400 iteration. Loss: 0.6726833581924438\n",
      "Training: 1 epoch. 4500 iteration. Loss: 2.1922965049743652\n",
      "Training: 1 epoch. 4600 iteration. Loss: 2.212397336959839\n",
      "Training: 1 epoch. 4700 iteration. Loss: 1.8878428936004639\n",
      "Training: 1 epoch. 4800 iteration. Loss: 1.1493364572525024\n",
      "Training: 1 epoch. 4900 iteration. Loss: 0.9678360223770142\n",
      "Training: 1 epoch. 5000 iteration. Loss: 1.6177515983581543\n",
      "Training: 1 epoch. 5100 iteration. Loss: 1.2415931224822998\n",
      "Training: 1 epoch. 5200 iteration. Loss: 0.9105167388916016\n",
      "Training: 1 epoch. 5300 iteration. Loss: 0.5568771362304688\n",
      "Training: 1 epoch. 5400 iteration. Loss: 1.798771858215332\n",
      "Training: 1 epoch. 5500 iteration. Loss: 0.9063978791236877\n",
      "Training: 1 epoch. 5600 iteration. Loss: 0.9918714761734009\n",
      "Training: 1 epoch. 5700 iteration. Loss: 2.9452877044677734\n",
      "Training: 1 epoch. 5800 iteration. Loss: 1.6124494075775146\n",
      "Training: 1 epoch. 5900 iteration. Loss: 1.3320350646972656\n",
      "Training: 1 epoch. 6000 iteration. Loss: 0.8267120718955994\n",
      "Training: 1 epoch. 6100 iteration. Loss: 0.9630677700042725\n",
      "Training: 1 epoch. 6200 iteration. Loss: 0.60053950548172\n",
      "Training: 1 epoch. 6300 iteration. Loss: 1.712045669555664\n",
      "Training: 1 epoch. 6400 iteration. Loss: 0.780445396900177\n",
      "Training: 1 epoch. 6500 iteration. Loss: 0.9336080551147461\n",
      "Training: 1 epoch. 6600 iteration. Loss: 1.3576204776763916\n",
      "Training: 1 epoch. 6700 iteration. Loss: 0.7022675275802612\n",
      "Training: 1 epoch. 6800 iteration. Loss: 1.685304880142212\n",
      "Training: 1 epoch. 6900 iteration. Loss: 0.9873635172843933\n",
      "Training: 1 epoch. 7000 iteration. Loss: 1.1811505556106567\n",
      "Training: 1 epoch. 7100 iteration. Loss: 1.1282446384429932\n",
      "Training: 1 epoch. 7200 iteration. Loss: 1.772371768951416\n",
      "Training: 1 epoch. 7300 iteration. Loss: 1.234375\n",
      "Training: 1 epoch. 7400 iteration. Loss: 0.45269763469696045\n",
      "Training: 1 epoch. 7500 iteration. Loss: 2.0696473121643066\n",
      "Training: 1 epoch. 7600 iteration. Loss: 0.8850928544998169\n",
      "Training: 1 epoch. 7700 iteration. Loss: 2.0750083923339844\n",
      "Training: 1 epoch. 7800 iteration. Loss: 0.7428455948829651\n",
      "Training: 1 epoch. 7900 iteration. Loss: 1.8658698797225952\n",
      "Training: 1 epoch. 8000 iteration. Loss: 0.873986542224884\n",
      "Training: 1 epoch. 8100 iteration. Loss: 1.7274469137191772\n",
      "Training: 1 epoch. 8200 iteration. Loss: 1.2066835165023804\n",
      "Training: 1 epoch. 8300 iteration. Loss: 2.4346346855163574\n",
      "Training: 1 epoch. 8400 iteration. Loss: 0.9135551452636719\n",
      "Training: 1 epoch. 8500 iteration. Loss: 1.4549463987350464\n",
      "Training: 1 epoch. 8600 iteration. Loss: 0.7211230993270874\n",
      "Training: 1 epoch. 8700 iteration. Loss: 0.8557803630828857\n",
      "Training: 1 epoch. 8800 iteration. Loss: 1.031293511390686\n",
      "Training: 1 epoch. 8900 iteration. Loss: 1.2423404455184937\n",
      "Training: 1 epoch. 9000 iteration. Loss: 1.3349015712738037\n",
      "Training: 1 epoch. 9100 iteration. Loss: 1.199668288230896\n",
      "Training: 1 epoch. 9200 iteration. Loss: 0.26047828793525696\n",
      "Training: 1 epoch. 9300 iteration. Loss: 1.0528756380081177\n",
      "Training: 1 epoch. 9400 iteration. Loss: 1.4867377281188965\n",
      "Training: 1 epoch. 9500 iteration. Loss: 1.5140618085861206\n",
      "Training: 1 epoch. 9600 iteration. Loss: 1.5146762132644653\n",
      "Training: 1 epoch. 9700 iteration. Loss: 1.2595564126968384\n",
      "Training: 1 epoch. 9800 iteration. Loss: 0.9417304396629333\n",
      "Training: 1 epoch. 9900 iteration. Loss: 1.3363261222839355\n",
      "Training: 1 epoch. 10000 iteration. Loss: 2.7159595489501953\n",
      "Training: 1 epoch. 10100 iteration. Loss: 1.1618103981018066\n",
      "Training: 1 epoch. 10200 iteration. Loss: 0.8184431791305542\n",
      "Training: 1 epoch. 10300 iteration. Loss: 1.908382534980774\n",
      "Training: 1 epoch. 10400 iteration. Loss: 0.6960811018943787\n",
      "Training: 1 epoch. 10500 iteration. Loss: 0.7040492296218872\n",
      "Training: 1 epoch. 10600 iteration. Loss: 1.4294686317443848\n",
      "Training: 1 epoch. 10700 iteration. Loss: 0.6032705307006836\n",
      "Training: 1 epoch. 10800 iteration. Loss: 0.6816977858543396\n",
      "Training: 1 epoch. 10900 iteration. Loss: 0.4998621940612793\n",
      "Training: 1 epoch. 11000 iteration. Loss: 0.6353816390037537\n",
      "Training: 1 epoch. 11100 iteration. Loss: 2.5257556438446045\n",
      "Training: 1 epoch. 11200 iteration. Loss: 1.2776576280593872\n",
      "Training: 1 epoch. 11300 iteration. Loss: 1.0908803939819336\n",
      "Training: 1 epoch. 11400 iteration. Loss: 0.17517909407615662\n",
      "Training: 1 epoch. 11500 iteration. Loss: 1.5034809112548828\n",
      "Training: 1 epoch. 11600 iteration. Loss: 0.6646005511283875\n",
      "Training: 1 epoch. 11700 iteration. Loss: 0.8908764719963074\n",
      "Training: 1 epoch. 11800 iteration. Loss: 1.3357994556427002\n",
      "Training: 1 epoch. 11900 iteration. Loss: 0.7595345377922058\n",
      "Training: 1 epoch. 12000 iteration. Loss: 0.6178485155105591\n",
      "Training: 1 epoch. 12100 iteration. Loss: 0.9556292295455933\n",
      "Training: 1 epoch. 12200 iteration. Loss: 1.180016040802002\n",
      "Training: 1 epoch. 12300 iteration. Loss: 0.6878537535667419\n",
      "Training: 1 epoch. 12400 iteration. Loss: 1.6135977506637573\n",
      "Training: 1 epoch. 12500 iteration. Loss: 1.0276119709014893\n",
      "Training loss (ave.): 1.414705378844738\n",
      "\n",
      "Validation start\n",
      "Validation loss: 4.688783416295052, Accuracy: 0.5889\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 2 epoch. 100 iteration. Loss: 0.9414774775505066\n",
      "Training: 2 epoch. 200 iteration. Loss: 0.9445465803146362\n",
      "Training: 2 epoch. 300 iteration. Loss: 0.9003355503082275\n",
      "Training: 2 epoch. 400 iteration. Loss: 0.5545027256011963\n",
      "Training: 2 epoch. 500 iteration. Loss: 1.0672996044158936\n",
      "Training: 2 epoch. 600 iteration. Loss: 0.6043223738670349\n",
      "Training: 2 epoch. 700 iteration. Loss: 1.8459678888320923\n",
      "Training: 2 epoch. 800 iteration. Loss: 0.611138105392456\n",
      "Training: 2 epoch. 900 iteration. Loss: 1.379560112953186\n",
      "Training: 2 epoch. 1000 iteration. Loss: 0.933193027973175\n",
      "Training: 2 epoch. 1100 iteration. Loss: 0.8682703971862793\n",
      "Training: 2 epoch. 1200 iteration. Loss: 0.7167418599128723\n",
      "Training: 2 epoch. 1300 iteration. Loss: 1.2657017707824707\n",
      "Training: 2 epoch. 1400 iteration. Loss: 1.4731358289718628\n",
      "Training: 2 epoch. 1500 iteration. Loss: 1.4989697933197021\n",
      "Training: 2 epoch. 1600 iteration. Loss: 0.5225452780723572\n",
      "Training: 2 epoch. 1700 iteration. Loss: 0.6406214237213135\n",
      "Training: 2 epoch. 1800 iteration. Loss: 1.2660636901855469\n",
      "Training: 2 epoch. 1900 iteration. Loss: 1.887016773223877\n",
      "Training: 2 epoch. 2000 iteration. Loss: 0.9776220917701721\n",
      "Training: 2 epoch. 2100 iteration. Loss: 1.9643440246582031\n",
      "Training: 2 epoch. 2200 iteration. Loss: 0.7754380106925964\n",
      "Training: 2 epoch. 2300 iteration. Loss: 0.4407002925872803\n",
      "Training: 2 epoch. 2400 iteration. Loss: 0.7513791918754578\n",
      "Training: 2 epoch. 2500 iteration. Loss: 1.3478868007659912\n",
      "Training: 2 epoch. 2600 iteration. Loss: 0.8835200071334839\n",
      "Training: 2 epoch. 2700 iteration. Loss: 0.7329698204994202\n",
      "Training: 2 epoch. 2800 iteration. Loss: 0.6199206709861755\n",
      "Training: 2 epoch. 2900 iteration. Loss: 1.062461018562317\n",
      "Training: 2 epoch. 3000 iteration. Loss: 0.6345679759979248\n",
      "Training: 2 epoch. 3100 iteration. Loss: 0.9690025448799133\n",
      "Training: 2 epoch. 3200 iteration. Loss: 0.5511417984962463\n",
      "Training: 2 epoch. 3300 iteration. Loss: 0.6442552804946899\n",
      "Training: 2 epoch. 3400 iteration. Loss: 0.995089054107666\n",
      "Training: 2 epoch. 3500 iteration. Loss: 1.022249698638916\n",
      "Training: 2 epoch. 3600 iteration. Loss: 0.740707516670227\n",
      "Training: 2 epoch. 3700 iteration. Loss: 0.8588734865188599\n",
      "Training: 2 epoch. 3800 iteration. Loss: 0.4862658381462097\n",
      "Training: 2 epoch. 3900 iteration. Loss: 1.6689833402633667\n",
      "Training: 2 epoch. 4000 iteration. Loss: 1.4511840343475342\n",
      "Training: 2 epoch. 4100 iteration. Loss: 2.5396742820739746\n",
      "Training: 2 epoch. 4200 iteration. Loss: 0.530752420425415\n",
      "Training: 2 epoch. 4300 iteration. Loss: 0.5569323301315308\n",
      "Training: 2 epoch. 4400 iteration. Loss: 0.7435598969459534\n",
      "Training: 2 epoch. 4500 iteration. Loss: 0.5198895931243896\n",
      "Training: 2 epoch. 4600 iteration. Loss: 1.2124704122543335\n",
      "Training: 2 epoch. 4700 iteration. Loss: 1.0531038045883179\n",
      "Training: 2 epoch. 4800 iteration. Loss: 0.812952995300293\n",
      "Training: 2 epoch. 4900 iteration. Loss: 1.3988908529281616\n",
      "Training: 2 epoch. 5000 iteration. Loss: 1.1442302465438843\n",
      "Training: 2 epoch. 5100 iteration. Loss: 0.3720153868198395\n",
      "Training: 2 epoch. 5200 iteration. Loss: 1.1103790998458862\n",
      "Training: 2 epoch. 5300 iteration. Loss: 1.2217828035354614\n",
      "Training: 2 epoch. 5400 iteration. Loss: 1.2682570219039917\n",
      "Training: 2 epoch. 5500 iteration. Loss: 1.9346364736557007\n",
      "Training: 2 epoch. 5600 iteration. Loss: 0.7913556098937988\n",
      "Training: 2 epoch. 5700 iteration. Loss: 1.7221579551696777\n",
      "Training: 2 epoch. 5800 iteration. Loss: 1.6150531768798828\n",
      "Training: 2 epoch. 5900 iteration. Loss: 1.772481918334961\n",
      "Training: 2 epoch. 6000 iteration. Loss: 1.159534215927124\n",
      "Training: 2 epoch. 6100 iteration. Loss: 0.7084787487983704\n",
      "Training: 2 epoch. 6200 iteration. Loss: 0.5817523002624512\n",
      "Training: 2 epoch. 6300 iteration. Loss: 0.45808690786361694\n",
      "Training: 2 epoch. 6400 iteration. Loss: 0.3159089982509613\n",
      "Training: 2 epoch. 6500 iteration. Loss: 0.40882551670074463\n",
      "Training: 2 epoch. 6600 iteration. Loss: 0.44360631704330444\n",
      "Training: 2 epoch. 6700 iteration. Loss: 1.3098084926605225\n",
      "Training: 2 epoch. 6800 iteration. Loss: 0.5444920659065247\n",
      "Training: 2 epoch. 6900 iteration. Loss: 1.5762970447540283\n",
      "Training: 2 epoch. 7000 iteration. Loss: 2.297882556915283\n",
      "Training: 2 epoch. 7100 iteration. Loss: 1.1294970512390137\n",
      "Training: 2 epoch. 7200 iteration. Loss: 0.8582570552825928\n",
      "Training: 2 epoch. 7300 iteration. Loss: 0.8884133100509644\n",
      "Training: 2 epoch. 7400 iteration. Loss: 1.0566229820251465\n",
      "Training: 2 epoch. 7500 iteration. Loss: 1.8587418794631958\n",
      "Training: 2 epoch. 7600 iteration. Loss: 2.1800050735473633\n",
      "Training: 2 epoch. 7700 iteration. Loss: 0.9242466688156128\n",
      "Training: 2 epoch. 7800 iteration. Loss: 1.3810393810272217\n",
      "Training: 2 epoch. 7900 iteration. Loss: 0.7677804827690125\n",
      "Training: 2 epoch. 8000 iteration. Loss: 0.6546496152877808\n",
      "Training: 2 epoch. 8100 iteration. Loss: 1.397618293762207\n",
      "Training: 2 epoch. 8200 iteration. Loss: 0.24398308992385864\n",
      "Training: 2 epoch. 8300 iteration. Loss: 0.9487828016281128\n",
      "Training: 2 epoch. 8400 iteration. Loss: 1.0753977298736572\n",
      "Training: 2 epoch. 8500 iteration. Loss: 0.4666005074977875\n",
      "Training: 2 epoch. 8600 iteration. Loss: 2.3465704917907715\n",
      "Training: 2 epoch. 8700 iteration. Loss: 0.7670738101005554\n",
      "Training: 2 epoch. 8800 iteration. Loss: 1.1730411052703857\n",
      "Training: 2 epoch. 8900 iteration. Loss: 1.1257182359695435\n",
      "Training: 2 epoch. 9000 iteration. Loss: 2.9524998664855957\n",
      "Training: 2 epoch. 9100 iteration. Loss: 0.5518558025360107\n",
      "Training: 2 epoch. 9200 iteration. Loss: 1.5923347473144531\n",
      "Training: 2 epoch. 9300 iteration. Loss: 1.0661654472351074\n",
      "Training: 2 epoch. 9400 iteration. Loss: 0.7115150690078735\n",
      "Training: 2 epoch. 9500 iteration. Loss: 1.9754213094711304\n",
      "Training: 2 epoch. 9600 iteration. Loss: 0.6689940094947815\n",
      "Training: 2 epoch. 9700 iteration. Loss: 1.8710566759109497\n",
      "Training: 2 epoch. 9800 iteration. Loss: 0.2089650183916092\n",
      "Training: 2 epoch. 9900 iteration. Loss: 0.10642145574092865\n",
      "Training: 2 epoch. 10000 iteration. Loss: 0.45973968505859375\n",
      "Training: 2 epoch. 10100 iteration. Loss: 1.1584831476211548\n",
      "Training: 2 epoch. 10200 iteration. Loss: 1.4753550291061401\n",
      "Training: 2 epoch. 10300 iteration. Loss: 0.19623982906341553\n",
      "Training: 2 epoch. 10400 iteration. Loss: 1.7946363687515259\n",
      "Training: 2 epoch. 10500 iteration. Loss: 1.7638299465179443\n",
      "Training: 2 epoch. 10600 iteration. Loss: 0.5280495882034302\n",
      "Training: 2 epoch. 10700 iteration. Loss: 1.4327034950256348\n",
      "Training: 2 epoch. 10800 iteration. Loss: 0.8055022358894348\n",
      "Training: 2 epoch. 10900 iteration. Loss: 0.7139575481414795\n",
      "Training: 2 epoch. 11000 iteration. Loss: 0.2533740997314453\n",
      "Training: 2 epoch. 11100 iteration. Loss: 1.2009789943695068\n",
      "Training: 2 epoch. 11200 iteration. Loss: 0.854821503162384\n",
      "Training: 2 epoch. 11300 iteration. Loss: 2.3205928802490234\n",
      "Training: 2 epoch. 11400 iteration. Loss: 0.5377949476242065\n",
      "Training: 2 epoch. 11500 iteration. Loss: 1.6500694751739502\n",
      "Training: 2 epoch. 11600 iteration. Loss: 3.1212010383605957\n",
      "Training: 2 epoch. 11700 iteration. Loss: 0.7015995979309082\n",
      "Training: 2 epoch. 11800 iteration. Loss: 0.8428323268890381\n",
      "Training: 2 epoch. 11900 iteration. Loss: 0.2322837859392166\n",
      "Training: 2 epoch. 12000 iteration. Loss: 2.0418455600738525\n",
      "Training: 2 epoch. 12100 iteration. Loss: 0.7099525928497314\n",
      "Training: 2 epoch. 12200 iteration. Loss: 1.5273287296295166\n",
      "Training: 2 epoch. 12300 iteration. Loss: 0.9779745936393738\n",
      "Training: 2 epoch. 12400 iteration. Loss: 0.8500584959983826\n",
      "Training: 2 epoch. 12500 iteration. Loss: 1.6121437549591064\n",
      "Training loss (ave.): 1.026344681543149\n",
      "\n",
      "Validation start\n",
      "Validation loss: 4.085891034656763, Accuracy: 0.6364\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 3 epoch. 100 iteration. Loss: 1.418303370475769\n",
      "Training: 3 epoch. 200 iteration. Loss: 0.6153450012207031\n",
      "Training: 3 epoch. 300 iteration. Loss: 0.47670820355415344\n",
      "Training: 3 epoch. 400 iteration. Loss: 1.5748984813690186\n",
      "Training: 3 epoch. 500 iteration. Loss: 0.08882801979780197\n",
      "Training: 3 epoch. 600 iteration. Loss: 0.5522828698158264\n",
      "Training: 3 epoch. 700 iteration. Loss: 0.5683016777038574\n",
      "Training: 3 epoch. 800 iteration. Loss: 0.741118311882019\n",
      "Training: 3 epoch. 900 iteration. Loss: 0.198794424533844\n",
      "Training: 3 epoch. 1000 iteration. Loss: 0.3734871745109558\n",
      "Training: 3 epoch. 1100 iteration. Loss: 1.1139662265777588\n",
      "Training: 3 epoch. 1200 iteration. Loss: 0.3543318510055542\n",
      "Training: 3 epoch. 1300 iteration. Loss: 0.9239339828491211\n",
      "Training: 3 epoch. 1400 iteration. Loss: 0.23910671472549438\n",
      "Training: 3 epoch. 1500 iteration. Loss: 0.1182439774274826\n",
      "Training: 3 epoch. 1600 iteration. Loss: 0.280700147151947\n",
      "Training: 3 epoch. 1700 iteration. Loss: 0.3950032889842987\n",
      "Training: 3 epoch. 1800 iteration. Loss: 0.5075578093528748\n",
      "Training: 3 epoch. 1900 iteration. Loss: 0.6330469250679016\n",
      "Training: 3 epoch. 2000 iteration. Loss: 0.5002893805503845\n",
      "Training: 3 epoch. 2100 iteration. Loss: 0.4116138219833374\n",
      "Training: 3 epoch. 2200 iteration. Loss: 1.7376788854599\n",
      "Training: 3 epoch. 2300 iteration. Loss: 2.162440299987793\n",
      "Training: 3 epoch. 2400 iteration. Loss: 0.4278983473777771\n",
      "Training: 3 epoch. 2500 iteration. Loss: 0.6803649067878723\n",
      "Training: 3 epoch. 2600 iteration. Loss: 0.4919668734073639\n",
      "Training: 3 epoch. 2700 iteration. Loss: 0.5057490468025208\n",
      "Training: 3 epoch. 2800 iteration. Loss: 1.5927627086639404\n",
      "Training: 3 epoch. 2900 iteration. Loss: 0.18436333537101746\n",
      "Training: 3 epoch. 3000 iteration. Loss: 0.5051822066307068\n",
      "Training: 3 epoch. 3100 iteration. Loss: 0.292675644159317\n",
      "Training: 3 epoch. 3200 iteration. Loss: 0.1946246474981308\n",
      "Training: 3 epoch. 3300 iteration. Loss: 0.10602658987045288\n",
      "Training: 3 epoch. 3400 iteration. Loss: 0.3986922800540924\n",
      "Training: 3 epoch. 3500 iteration. Loss: 0.48328882455825806\n",
      "Training: 3 epoch. 3600 iteration. Loss: 0.45616012811660767\n",
      "Training: 3 epoch. 3700 iteration. Loss: 1.2029025554656982\n",
      "Training: 3 epoch. 3800 iteration. Loss: 0.2238501012325287\n",
      "Training: 3 epoch. 3900 iteration. Loss: 1.1756824254989624\n",
      "Training: 3 epoch. 4000 iteration. Loss: 1.486809492111206\n",
      "Training: 3 epoch. 4100 iteration. Loss: 1.0378732681274414\n",
      "Training: 3 epoch. 4200 iteration. Loss: 1.598032832145691\n",
      "Training: 3 epoch. 4300 iteration. Loss: 1.6013103723526\n",
      "Training: 3 epoch. 4400 iteration. Loss: 1.0571315288543701\n",
      "Training: 3 epoch. 4500 iteration. Loss: 1.7870651483535767\n",
      "Training: 3 epoch. 4600 iteration. Loss: 0.7324081063270569\n",
      "Training: 3 epoch. 4700 iteration. Loss: 0.5575958490371704\n",
      "Training: 3 epoch. 4800 iteration. Loss: 1.0681923627853394\n",
      "Training: 3 epoch. 4900 iteration. Loss: 1.7040154933929443\n",
      "Training: 3 epoch. 5000 iteration. Loss: 0.7288010120391846\n",
      "Training: 3 epoch. 5100 iteration. Loss: 0.5417516827583313\n",
      "Training: 3 epoch. 5200 iteration. Loss: 0.7545766234397888\n",
      "Training: 3 epoch. 5300 iteration. Loss: 1.064194917678833\n",
      "Training: 3 epoch. 5400 iteration. Loss: 0.18884889781475067\n",
      "Training: 3 epoch. 5500 iteration. Loss: 1.0677920579910278\n",
      "Training: 3 epoch. 5600 iteration. Loss: 1.1437647342681885\n",
      "Training: 3 epoch. 5700 iteration. Loss: 1.1710901260375977\n",
      "Training: 3 epoch. 5800 iteration. Loss: 0.46053487062454224\n",
      "Training: 3 epoch. 5900 iteration. Loss: 0.16362658143043518\n",
      "Training: 3 epoch. 6000 iteration. Loss: 0.6992629766464233\n",
      "Training: 3 epoch. 6100 iteration. Loss: 0.7808411121368408\n",
      "Training: 3 epoch. 6200 iteration. Loss: 1.223022222518921\n",
      "Training: 3 epoch. 6300 iteration. Loss: 0.8251793384552002\n",
      "Training: 3 epoch. 6400 iteration. Loss: 0.6016960740089417\n",
      "Training: 3 epoch. 6500 iteration. Loss: 0.5394670963287354\n",
      "Training: 3 epoch. 6600 iteration. Loss: 0.5463286638259888\n",
      "Training: 3 epoch. 6700 iteration. Loss: 0.9192678332328796\n",
      "Training: 3 epoch. 6800 iteration. Loss: 1.763698697090149\n",
      "Training: 3 epoch. 6900 iteration. Loss: 1.1372573375701904\n",
      "Training: 3 epoch. 7000 iteration. Loss: 1.0969856977462769\n",
      "Training: 3 epoch. 7100 iteration. Loss: 1.0789498090744019\n",
      "Training: 3 epoch. 7200 iteration. Loss: 0.2546367943286896\n",
      "Training: 3 epoch. 7300 iteration. Loss: 2.274061679840088\n",
      "Training: 3 epoch. 7400 iteration. Loss: 0.5880701541900635\n",
      "Training: 3 epoch. 7500 iteration. Loss: 1.5392577648162842\n",
      "Training: 3 epoch. 7600 iteration. Loss: 0.2925972044467926\n",
      "Training: 3 epoch. 7700 iteration. Loss: 0.6841838955879211\n",
      "Training: 3 epoch. 7800 iteration. Loss: 0.45868051052093506\n",
      "Training: 3 epoch. 7900 iteration. Loss: 1.5166146755218506\n",
      "Training: 3 epoch. 8000 iteration. Loss: 1.1665855646133423\n",
      "Training: 3 epoch. 8100 iteration. Loss: 0.8092091083526611\n",
      "Training: 3 epoch. 8200 iteration. Loss: 1.0149809122085571\n",
      "Training: 3 epoch. 8300 iteration. Loss: 1.5638779401779175\n",
      "Training: 3 epoch. 8400 iteration. Loss: 0.4448084235191345\n",
      "Training: 3 epoch. 8500 iteration. Loss: 1.0154802799224854\n",
      "Training: 3 epoch. 8600 iteration. Loss: 1.0594017505645752\n",
      "Training: 3 epoch. 8700 iteration. Loss: 1.0867109298706055\n",
      "Training: 3 epoch. 8800 iteration. Loss: 0.6659123301506042\n",
      "Training: 3 epoch. 8900 iteration. Loss: 1.0974829196929932\n",
      "Training: 3 epoch. 9000 iteration. Loss: 0.9734035730361938\n",
      "Training: 3 epoch. 9100 iteration. Loss: 0.5859056711196899\n",
      "Training: 3 epoch. 9200 iteration. Loss: 0.2877940535545349\n",
      "Training: 3 epoch. 9300 iteration. Loss: 1.1204644441604614\n",
      "Training: 3 epoch. 9400 iteration. Loss: 1.33356773853302\n",
      "Training: 3 epoch. 9500 iteration. Loss: 1.2945702075958252\n",
      "Training: 3 epoch. 9600 iteration. Loss: 0.45708563923835754\n",
      "Training: 3 epoch. 9700 iteration. Loss: 1.6702892780303955\n",
      "Training: 3 epoch. 9800 iteration. Loss: 0.18928201496601105\n",
      "Training: 3 epoch. 9900 iteration. Loss: 1.6079437732696533\n",
      "Training: 3 epoch. 10000 iteration. Loss: 1.058526873588562\n",
      "Training: 3 epoch. 10100 iteration. Loss: 0.5799967050552368\n",
      "Training: 3 epoch. 10200 iteration. Loss: 0.3522871732711792\n",
      "Training: 3 epoch. 10300 iteration. Loss: 0.3474114239215851\n",
      "Training: 3 epoch. 10400 iteration. Loss: 0.7903677225112915\n",
      "Training: 3 epoch. 10500 iteration. Loss: 1.4230749607086182\n",
      "Training: 3 epoch. 10600 iteration. Loss: 1.1166841983795166\n",
      "Training: 3 epoch. 10700 iteration. Loss: 0.1384202539920807\n",
      "Training: 3 epoch. 10800 iteration. Loss: 0.3797737956047058\n",
      "Training: 3 epoch. 10900 iteration. Loss: 1.4758626222610474\n",
      "Training: 3 epoch. 11000 iteration. Loss: 0.8347924947738647\n",
      "Training: 3 epoch. 11100 iteration. Loss: 0.5275236964225769\n",
      "Training: 3 epoch. 11200 iteration. Loss: 2.7672317028045654\n",
      "Training: 3 epoch. 11300 iteration. Loss: 0.2488938271999359\n",
      "Training: 3 epoch. 11400 iteration. Loss: 1.1014211177825928\n",
      "Training: 3 epoch. 11500 iteration. Loss: 1.7076525688171387\n",
      "Training: 3 epoch. 11600 iteration. Loss: 0.24722881615161896\n",
      "Training: 3 epoch. 11700 iteration. Loss: 0.5668303370475769\n",
      "Training: 3 epoch. 11800 iteration. Loss: 1.2308717966079712\n",
      "Training: 3 epoch. 11900 iteration. Loss: 0.2862793803215027\n",
      "Training: 3 epoch. 12000 iteration. Loss: 0.3593383729457855\n",
      "Training: 3 epoch. 12100 iteration. Loss: 0.4429910480976105\n",
      "Training: 3 epoch. 12200 iteration. Loss: 0.21953067183494568\n",
      "Training: 3 epoch. 12300 iteration. Loss: 0.527145266532898\n",
      "Training: 3 epoch. 12400 iteration. Loss: 0.03602743148803711\n",
      "Training: 3 epoch. 12500 iteration. Loss: 0.8642057180404663\n",
      "Training loss (ave.): 0.8632307318442315\n",
      "\n",
      "Validation start\n",
      "Validation loss: 3.7996210366487504, Accuracy: 0.6759\n",
      "\n",
      "0.99993241\n",
      "{'train_loss': [1.414705378844738, 1.026344681543149, 0.8632307318442315], 'validation_loss': [4.688783416295052, 4.085891034656763, 3.7996210366487504], 'validation_acc': [0.5889, 0.6364, 0.6759]}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/naistrll/Parameter_tuning/src/CIFAR10img/loss.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Parameter_tuning/src/CIFAR10/CIFAR_train.py:204\u001b[0m\n\u001b[1;32m    202\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m--> 204\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg/loss.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m    207\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:859\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    858\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m--> 859\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()   \u001b[38;5;66;03m# need this if 'transparent=True' to reset colors\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py:2311\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2308\u001b[0m         patch\u001b[38;5;241m.\u001b[39mset_facecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2309\u001b[0m         patch\u001b[38;5;241m.\u001b[39mset_edgecolor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transparent:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax, cc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, original_axes_colors):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2210\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     _bbox_inches_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2210\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2214\u001b[0m \u001b[43m        \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2215\u001b[0m \u001b[43m        \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:1639\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m     cbook\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1632\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.3\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   1633\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m() got unexpected keyword argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1634\u001b[0m                 \u001b[38;5;241m+\u001b[39m arg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m which is no longer supported as of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1635\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m and will become an error \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1636\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1637\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1639\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:510\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03mWrite the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 510\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:1611\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1610\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1611\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:2350\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2348\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2349\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2350\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2353\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/naistrll/Parameter_tuning/src/CIFAR10img/loss.png'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run ../src/CIFAR10/CIFAR_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b814573-d881-4dc3-a625-6c5f537882a4",
   "metadata": {},
   "source": [
    "# Tensorboardでの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c662e6f-92d7-4903-b4b9-b38acb1ae4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 14:05:56.305153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.29' not found (required by /opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /opt/conda/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "E0712 14:05:57.105607 140255621554304 program.py:298] TensorBoard could not bind to port 6001, it was already in use\n",
      "ERROR: TensorBoard could not bind to port 6001, it was already in use\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ../logs/CIFAR10 --port 6001 --bind_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1367156-d5c2-4212-91b5-b39366f9f777",
   "metadata": {},
   "source": [
    "# パラメータの変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e589346-9195-44ad-8d42-09602707cbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 1 epoch. 100 iteration. Loss: 2.019185781478882\n",
      "Training: 1 epoch. 200 iteration. Loss: 2.2167959213256836\n",
      "Training: 1 epoch. 300 iteration. Loss: 1.9780528545379639\n",
      "Training: 1 epoch. 400 iteration. Loss: 1.9798051118850708\n",
      "Training: 1 epoch. 500 iteration. Loss: 1.6303026676177979\n",
      "Training: 1 epoch. 600 iteration. Loss: 2.15621280670166\n",
      "Training: 1 epoch. 700 iteration. Loss: 2.0492937564849854\n",
      "Training: 1 epoch. 800 iteration. Loss: 1.8930000066757202\n",
      "Training: 1 epoch. 900 iteration. Loss: 1.7002995014190674\n",
      "Training: 1 epoch. 1000 iteration. Loss: 2.2906904220581055\n",
      "Training: 1 epoch. 1100 iteration. Loss: 1.838094711303711\n",
      "Training: 1 epoch. 1200 iteration. Loss: 2.144456148147583\n",
      "Training: 1 epoch. 1300 iteration. Loss: 2.624091625213623\n",
      "Training: 1 epoch. 1400 iteration. Loss: 2.2867066860198975\n",
      "Training: 1 epoch. 1500 iteration. Loss: 1.8463895320892334\n",
      "Training: 1 epoch. 1600 iteration. Loss: 2.2623016834259033\n",
      "Training: 1 epoch. 1700 iteration. Loss: 1.5666964054107666\n",
      "Training: 1 epoch. 1800 iteration. Loss: 1.1506314277648926\n",
      "Training: 1 epoch. 1900 iteration. Loss: 1.6159839630126953\n",
      "Training: 1 epoch. 2000 iteration. Loss: 1.2414233684539795\n",
      "Training: 1 epoch. 2100 iteration. Loss: 1.4864277839660645\n",
      "Training: 1 epoch. 2200 iteration. Loss: 1.8999000787734985\n",
      "Training: 1 epoch. 2300 iteration. Loss: 1.259106159210205\n",
      "Training: 1 epoch. 2400 iteration. Loss: 1.8044703006744385\n",
      "Training: 1 epoch. 2500 iteration. Loss: 2.6370630264282227\n",
      "Training: 1 epoch. 2600 iteration. Loss: 0.982135534286499\n",
      "Training: 1 epoch. 2700 iteration. Loss: 1.158610224723816\n",
      "Training: 1 epoch. 2800 iteration. Loss: 1.1090822219848633\n",
      "Training: 1 epoch. 2900 iteration. Loss: 1.238957166671753\n",
      "Training: 1 epoch. 3000 iteration. Loss: 1.6068058013916016\n",
      "Training: 1 epoch. 3100 iteration. Loss: 1.63041090965271\n",
      "Training: 1 epoch. 3200 iteration. Loss: 2.602850914001465\n",
      "Training: 1 epoch. 3300 iteration. Loss: 1.263318419456482\n",
      "Training: 1 epoch. 3400 iteration. Loss: 1.8577275276184082\n",
      "Training: 1 epoch. 3500 iteration. Loss: 1.4108023643493652\n",
      "Training: 1 epoch. 3600 iteration. Loss: 1.6900547742843628\n",
      "Training: 1 epoch. 3700 iteration. Loss: 1.0539295673370361\n",
      "Training: 1 epoch. 3800 iteration. Loss: 1.448425531387329\n",
      "Training: 1 epoch. 3900 iteration. Loss: 1.131524920463562\n",
      "Training: 1 epoch. 4000 iteration. Loss: 0.8010080456733704\n",
      "Training: 1 epoch. 4100 iteration. Loss: 1.67057204246521\n",
      "Training: 1 epoch. 4200 iteration. Loss: 1.2209234237670898\n",
      "Training: 1 epoch. 4300 iteration. Loss: 1.4638502597808838\n",
      "Training: 1 epoch. 4400 iteration. Loss: 2.1848511695861816\n",
      "Training: 1 epoch. 4500 iteration. Loss: 1.2404975891113281\n",
      "Training: 1 epoch. 4600 iteration. Loss: 1.9846158027648926\n",
      "Training: 1 epoch. 4700 iteration. Loss: 1.342339277267456\n",
      "Training: 1 epoch. 4800 iteration. Loss: 1.6284997463226318\n",
      "Training: 1 epoch. 4900 iteration. Loss: 1.7644120454788208\n",
      "Training: 1 epoch. 5000 iteration. Loss: 0.8646389842033386\n",
      "Training: 1 epoch. 5100 iteration. Loss: 1.3897349834442139\n",
      "Training: 1 epoch. 5200 iteration. Loss: 1.6403361558914185\n",
      "Training: 1 epoch. 5300 iteration. Loss: 1.2409948110580444\n",
      "Training: 1 epoch. 5400 iteration. Loss: 1.2825205326080322\n",
      "Training: 1 epoch. 5500 iteration. Loss: 0.60981285572052\n",
      "Training: 1 epoch. 5600 iteration. Loss: 1.644291877746582\n",
      "Training: 1 epoch. 5700 iteration. Loss: 1.3401765823364258\n",
      "Training: 1 epoch. 5800 iteration. Loss: 0.8992160558700562\n",
      "Training: 1 epoch. 5900 iteration. Loss: 2.2943575382232666\n",
      "Training: 1 epoch. 6000 iteration. Loss: 0.6473703384399414\n",
      "Training: 1 epoch. 6100 iteration. Loss: 1.0231468677520752\n",
      "Training: 1 epoch. 6200 iteration. Loss: 0.7206963300704956\n",
      "Training: 1 epoch. 6300 iteration. Loss: 2.5675668716430664\n",
      "Training: 1 epoch. 6400 iteration. Loss: 1.2671014070510864\n",
      "Training: 1 epoch. 6500 iteration. Loss: 1.6368842124938965\n",
      "Training: 1 epoch. 6600 iteration. Loss: 0.5687799453735352\n",
      "Training: 1 epoch. 6700 iteration. Loss: 0.7118200659751892\n",
      "Training: 1 epoch. 6800 iteration. Loss: 1.6412019729614258\n",
      "Training: 1 epoch. 6900 iteration. Loss: 1.2296959161758423\n",
      "Training: 1 epoch. 7000 iteration. Loss: 0.958623468875885\n",
      "Training: 1 epoch. 7100 iteration. Loss: 1.369295358657837\n",
      "Training: 1 epoch. 7200 iteration. Loss: 1.072190284729004\n",
      "Training: 1 epoch. 7300 iteration. Loss: 1.7723337411880493\n",
      "Training: 1 epoch. 7400 iteration. Loss: 1.3536081314086914\n",
      "Training: 1 epoch. 7500 iteration. Loss: 2.459967613220215\n",
      "Training: 1 epoch. 7600 iteration. Loss: 1.3211817741394043\n",
      "Training: 1 epoch. 7700 iteration. Loss: 1.5397700071334839\n",
      "Training: 1 epoch. 7800 iteration. Loss: 1.3606969118118286\n",
      "Training: 1 epoch. 7900 iteration. Loss: 1.2611281871795654\n",
      "Training: 1 epoch. 8000 iteration. Loss: 1.3239210844039917\n",
      "Training: 1 epoch. 8100 iteration. Loss: 1.9204800128936768\n",
      "Training: 1 epoch. 8200 iteration. Loss: 1.6403213739395142\n",
      "Training: 1 epoch. 8300 iteration. Loss: 2.1343941688537598\n",
      "Training: 1 epoch. 8400 iteration. Loss: 1.9841976165771484\n",
      "Training: 1 epoch. 8500 iteration. Loss: 0.7809637784957886\n",
      "Training: 1 epoch. 8600 iteration. Loss: 1.210648775100708\n",
      "Training: 1 epoch. 8700 iteration. Loss: 1.0973238945007324\n",
      "Training: 1 epoch. 8800 iteration. Loss: 1.2882269620895386\n",
      "Training: 1 epoch. 8900 iteration. Loss: 1.4305939674377441\n",
      "Training: 1 epoch. 9000 iteration. Loss: 0.8729888796806335\n",
      "Training: 1 epoch. 9100 iteration. Loss: 1.1755173206329346\n",
      "Training: 1 epoch. 9200 iteration. Loss: 1.5453130006790161\n",
      "Training: 1 epoch. 9300 iteration. Loss: 0.31650906801223755\n",
      "Training: 1 epoch. 9400 iteration. Loss: 1.2945115566253662\n",
      "Training: 1 epoch. 9500 iteration. Loss: 1.0716432332992554\n",
      "Training: 1 epoch. 9600 iteration. Loss: 1.0691804885864258\n",
      "Training: 1 epoch. 9700 iteration. Loss: 0.8618791103363037\n",
      "Training: 1 epoch. 9800 iteration. Loss: 0.9448859691619873\n",
      "Training: 1 epoch. 9900 iteration. Loss: 0.930008590221405\n",
      "Training: 1 epoch. 10000 iteration. Loss: 1.2257425785064697\n",
      "Training: 1 epoch. 10100 iteration. Loss: 1.7481064796447754\n",
      "Training: 1 epoch. 10200 iteration. Loss: 2.069986343383789\n",
      "Training: 1 epoch. 10300 iteration. Loss: 0.5552722215652466\n",
      "Training: 1 epoch. 10400 iteration. Loss: 0.9476014971733093\n",
      "Training: 1 epoch. 10500 iteration. Loss: 0.2600170075893402\n",
      "Training: 1 epoch. 10600 iteration. Loss: 0.7431669235229492\n",
      "Training: 1 epoch. 10700 iteration. Loss: 1.2797536849975586\n",
      "Training: 1 epoch. 10800 iteration. Loss: 2.720245361328125\n",
      "Training: 1 epoch. 10900 iteration. Loss: 0.6488372087478638\n",
      "Training: 1 epoch. 11000 iteration. Loss: 1.2636444568634033\n",
      "Training: 1 epoch. 11100 iteration. Loss: 1.1915392875671387\n",
      "Training: 1 epoch. 11200 iteration. Loss: 1.2850252389907837\n",
      "Training: 1 epoch. 11300 iteration. Loss: 1.3537591695785522\n",
      "Training: 1 epoch. 11400 iteration. Loss: 0.8942703008651733\n",
      "Training: 1 epoch. 11500 iteration. Loss: 0.2926112711429596\n",
      "Training: 1 epoch. 11600 iteration. Loss: 0.3604370057582855\n",
      "Training: 1 epoch. 11700 iteration. Loss: 1.2146716117858887\n",
      "Training: 1 epoch. 11800 iteration. Loss: 1.6724143028259277\n",
      "Training: 1 epoch. 11900 iteration. Loss: 0.6290793418884277\n",
      "Training: 1 epoch. 12000 iteration. Loss: 0.8801062703132629\n",
      "Training: 1 epoch. 12100 iteration. Loss: 0.9611672759056091\n",
      "Training: 1 epoch. 12200 iteration. Loss: 0.9980359077453613\n",
      "Training: 1 epoch. 12300 iteration. Loss: 1.4274535179138184\n",
      "Training: 1 epoch. 12400 iteration. Loss: 1.376535415649414\n",
      "Training: 1 epoch. 12500 iteration. Loss: 0.9698599576950073\n",
      "Training loss (ave.): 1.4270725371474027\n",
      "\n",
      "Validation start\n",
      "Validation loss: 4.748206188350916, Accuracy: 0.5817\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 2 epoch. 100 iteration. Loss: 1.2989641427993774\n",
      "Training: 2 epoch. 200 iteration. Loss: 0.5145196914672852\n",
      "Training: 2 epoch. 300 iteration. Loss: 1.1677902936935425\n",
      "Training: 2 epoch. 400 iteration. Loss: 1.5064363479614258\n",
      "Training: 2 epoch. 500 iteration. Loss: 1.4999656677246094\n",
      "Training: 2 epoch. 600 iteration. Loss: 2.143087387084961\n",
      "Training: 2 epoch. 700 iteration. Loss: 0.5995321273803711\n",
      "Training: 2 epoch. 800 iteration. Loss: 0.8161089420318604\n",
      "Training: 2 epoch. 900 iteration. Loss: 0.43760180473327637\n",
      "Training: 2 epoch. 1000 iteration. Loss: 1.7059383392333984\n",
      "Training: 2 epoch. 1100 iteration. Loss: 1.8123743534088135\n",
      "Training: 2 epoch. 1200 iteration. Loss: 0.3803814947605133\n",
      "Training: 2 epoch. 1300 iteration. Loss: 1.0017552375793457\n",
      "Training: 2 epoch. 1400 iteration. Loss: 2.3412973880767822\n",
      "Training: 2 epoch. 1500 iteration. Loss: 0.7199015617370605\n",
      "Training: 2 epoch. 1600 iteration. Loss: 0.6537799835205078\n",
      "Training: 2 epoch. 1700 iteration. Loss: 1.0038899183273315\n",
      "Training: 2 epoch. 1800 iteration. Loss: 1.0830190181732178\n",
      "Training: 2 epoch. 1900 iteration. Loss: 0.640689492225647\n",
      "Training: 2 epoch. 2000 iteration. Loss: 0.22673127055168152\n",
      "Training: 2 epoch. 2100 iteration. Loss: 0.6385374069213867\n",
      "Training: 2 epoch. 2200 iteration. Loss: 2.645293951034546\n",
      "Training: 2 epoch. 2300 iteration. Loss: 1.4130120277404785\n",
      "Training: 2 epoch. 2400 iteration. Loss: 0.8507410287857056\n",
      "Training: 2 epoch. 2500 iteration. Loss: 0.6857315301895142\n",
      "Training: 2 epoch. 2600 iteration. Loss: 2.7619881629943848\n",
      "Training: 2 epoch. 2700 iteration. Loss: 1.3796610832214355\n",
      "Training: 2 epoch. 2800 iteration. Loss: 1.1757549047470093\n",
      "Training: 2 epoch. 2900 iteration. Loss: 1.5192193984985352\n",
      "Training: 2 epoch. 3000 iteration. Loss: 1.0873920917510986\n",
      "Training: 2 epoch. 3100 iteration. Loss: 0.20581863820552826\n",
      "Training: 2 epoch. 3200 iteration. Loss: 1.5289585590362549\n",
      "Training: 2 epoch. 3300 iteration. Loss: 1.2446209192276\n",
      "Training: 2 epoch. 3400 iteration. Loss: 0.5242266654968262\n",
      "Training: 2 epoch. 3500 iteration. Loss: 1.4064364433288574\n",
      "Training: 2 epoch. 3600 iteration. Loss: 2.131364107131958\n",
      "Training: 2 epoch. 3700 iteration. Loss: 0.7391786575317383\n",
      "Training: 2 epoch. 3800 iteration. Loss: 0.693338930606842\n",
      "Training: 2 epoch. 3900 iteration. Loss: 1.4972896575927734\n",
      "Training: 2 epoch. 4000 iteration. Loss: 0.6658768653869629\n",
      "Training: 2 epoch. 4100 iteration. Loss: 1.3431177139282227\n",
      "Training: 2 epoch. 4200 iteration. Loss: 1.9543766975402832\n",
      "Training: 2 epoch. 4300 iteration. Loss: 1.0434019565582275\n",
      "Training: 2 epoch. 4400 iteration. Loss: 0.6918649673461914\n",
      "Training: 2 epoch. 4500 iteration. Loss: 0.37640345096588135\n",
      "Training: 2 epoch. 4600 iteration. Loss: 0.6232052445411682\n",
      "Training: 2 epoch. 4700 iteration. Loss: 0.16937875747680664\n",
      "Training: 2 epoch. 4800 iteration. Loss: 0.6012043952941895\n",
      "Training: 2 epoch. 4900 iteration. Loss: 0.7311804890632629\n",
      "Training: 2 epoch. 5000 iteration. Loss: 0.6036610007286072\n",
      "Training: 2 epoch. 5100 iteration. Loss: 1.6741554737091064\n",
      "Training: 2 epoch. 5200 iteration. Loss: 0.36574798822402954\n",
      "Training: 2 epoch. 5300 iteration. Loss: 0.485275000333786\n",
      "Training: 2 epoch. 5400 iteration. Loss: 0.8714935779571533\n",
      "Training: 2 epoch. 5500 iteration. Loss: 0.33026421070098877\n",
      "Training: 2 epoch. 5600 iteration. Loss: 0.4971473813056946\n",
      "Training: 2 epoch. 5700 iteration. Loss: 0.7010916471481323\n",
      "Training: 2 epoch. 5800 iteration. Loss: 0.5297098159790039\n",
      "Training: 2 epoch. 5900 iteration. Loss: 1.4952592849731445\n",
      "Training: 2 epoch. 6000 iteration. Loss: 0.9404109716415405\n",
      "Training: 2 epoch. 6100 iteration. Loss: 1.2506603002548218\n",
      "Training: 2 epoch. 6200 iteration. Loss: 0.6650464534759521\n",
      "Training: 2 epoch. 6300 iteration. Loss: 1.019383430480957\n",
      "Training: 2 epoch. 6400 iteration. Loss: 1.4432703256607056\n",
      "Training: 2 epoch. 6500 iteration. Loss: 0.42138975858688354\n",
      "Training: 2 epoch. 6600 iteration. Loss: 0.17228387296199799\n",
      "Training: 2 epoch. 6700 iteration. Loss: 0.9184558987617493\n",
      "Training: 2 epoch. 6800 iteration. Loss: 1.2034400701522827\n",
      "Training: 2 epoch. 6900 iteration. Loss: 1.923169493675232\n",
      "Training: 2 epoch. 7000 iteration. Loss: 1.1083998680114746\n",
      "Training: 2 epoch. 7100 iteration. Loss: 0.8670889139175415\n",
      "Training: 2 epoch. 7200 iteration. Loss: 1.4383689165115356\n",
      "Training: 2 epoch. 7300 iteration. Loss: 0.7422747611999512\n",
      "Training: 2 epoch. 7400 iteration. Loss: 1.4732601642608643\n",
      "Training: 2 epoch. 7500 iteration. Loss: 1.9321694374084473\n",
      "Training: 2 epoch. 7600 iteration. Loss: 2.841872215270996\n",
      "Training: 2 epoch. 7700 iteration. Loss: 0.4467686414718628\n",
      "Training: 2 epoch. 7800 iteration. Loss: 0.6751308441162109\n",
      "Training: 2 epoch. 7900 iteration. Loss: 0.6886497139930725\n",
      "Training: 2 epoch. 8000 iteration. Loss: 0.40405112504959106\n",
      "Training: 2 epoch. 8100 iteration. Loss: 1.0325146913528442\n",
      "Training: 2 epoch. 8200 iteration. Loss: 0.6267480254173279\n",
      "Training: 2 epoch. 8300 iteration. Loss: 0.21812504529953003\n",
      "Training: 2 epoch. 8400 iteration. Loss: 1.554572582244873\n",
      "Training: 2 epoch. 8500 iteration. Loss: 0.9059694409370422\n",
      "Training: 2 epoch. 8600 iteration. Loss: 0.7533186674118042\n",
      "Training: 2 epoch. 8700 iteration. Loss: 1.167458415031433\n",
      "Training: 2 epoch. 8800 iteration. Loss: 1.0305379629135132\n",
      "Training: 2 epoch. 8900 iteration. Loss: 0.797642707824707\n",
      "Training: 2 epoch. 9000 iteration. Loss: 0.6737918257713318\n",
      "Training: 2 epoch. 9100 iteration. Loss: 1.1063121557235718\n",
      "Training: 2 epoch. 9200 iteration. Loss: 0.266603946685791\n",
      "Training: 2 epoch. 9300 iteration. Loss: 1.0804808139801025\n",
      "Training: 2 epoch. 9400 iteration. Loss: 0.6651259064674377\n",
      "Training: 2 epoch. 9500 iteration. Loss: 1.4544025659561157\n",
      "Training: 2 epoch. 9600 iteration. Loss: 0.10684925317764282\n",
      "Training: 2 epoch. 9700 iteration. Loss: 0.7516498565673828\n",
      "Training: 2 epoch. 9800 iteration. Loss: 0.9142155647277832\n",
      "Training: 2 epoch. 9900 iteration. Loss: 0.3815184533596039\n",
      "Training: 2 epoch. 10000 iteration. Loss: 0.6926761865615845\n",
      "Training: 2 epoch. 10100 iteration. Loss: 0.12111343443393707\n",
      "Training: 2 epoch. 10200 iteration. Loss: 1.079136610031128\n",
      "Training: 2 epoch. 10300 iteration. Loss: 0.6648805141448975\n",
      "Training: 2 epoch. 10400 iteration. Loss: 1.6303752660751343\n",
      "Training: 2 epoch. 10500 iteration. Loss: 0.9076114892959595\n",
      "Training: 2 epoch. 10600 iteration. Loss: 0.7318401336669922\n",
      "Training: 2 epoch. 10700 iteration. Loss: 1.6815309524536133\n",
      "Training: 2 epoch. 10800 iteration. Loss: 1.3738375902175903\n",
      "Training: 2 epoch. 10900 iteration. Loss: 1.3845115900039673\n",
      "Training: 2 epoch. 11000 iteration. Loss: 0.6581821441650391\n",
      "Training: 2 epoch. 11100 iteration. Loss: 1.2665683031082153\n",
      "Training: 2 epoch. 11200 iteration. Loss: 0.2462969869375229\n",
      "Training: 2 epoch. 11300 iteration. Loss: 1.0717486143112183\n",
      "Training: 2 epoch. 11400 iteration. Loss: 1.6897788047790527\n",
      "Training: 2 epoch. 11500 iteration. Loss: 0.2898055613040924\n",
      "Training: 2 epoch. 11600 iteration. Loss: 0.17872042953968048\n",
      "Training: 2 epoch. 11700 iteration. Loss: 2.4393444061279297\n",
      "Training: 2 epoch. 11800 iteration. Loss: 1.9868135452270508\n",
      "Training: 2 epoch. 11900 iteration. Loss: 0.9492243528366089\n",
      "Training: 2 epoch. 12000 iteration. Loss: 0.5559630990028381\n",
      "Training: 2 epoch. 12100 iteration. Loss: 1.0270146131515503\n",
      "Training: 2 epoch. 12200 iteration. Loss: 0.8668313026428223\n",
      "Training: 2 epoch. 12300 iteration. Loss: 0.6121741533279419\n",
      "Training: 2 epoch. 12400 iteration. Loss: 1.0668166875839233\n",
      "Training: 2 epoch. 12500 iteration. Loss: 0.02607559598982334\n",
      "Training loss (ave.): 1.0010304063986242\n",
      "\n",
      "Validation start\n",
      "Validation loss: 3.748224208378792, Accuracy: 0.6734\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 3 epoch. 100 iteration. Loss: 0.6709445118904114\n",
      "Training: 3 epoch. 200 iteration. Loss: 0.4191132187843323\n",
      "Training: 3 epoch. 300 iteration. Loss: 0.21393312513828278\n",
      "Training: 3 epoch. 400 iteration. Loss: 0.22415779531002045\n",
      "Training: 3 epoch. 500 iteration. Loss: 0.39383459091186523\n",
      "Training: 3 epoch. 600 iteration. Loss: 1.2506097555160522\n",
      "Training: 3 epoch. 700 iteration. Loss: 2.711540699005127\n",
      "Training: 3 epoch. 800 iteration. Loss: 0.7996066808700562\n",
      "Training: 3 epoch. 900 iteration. Loss: 0.9094001054763794\n",
      "Training: 3 epoch. 1000 iteration. Loss: 0.9081848859786987\n",
      "Training: 3 epoch. 1100 iteration. Loss: 0.6608150005340576\n",
      "Training: 3 epoch. 1200 iteration. Loss: 0.11874284595251083\n",
      "Training: 3 epoch. 1300 iteration. Loss: 1.4408750534057617\n",
      "Training: 3 epoch. 1400 iteration. Loss: 0.36566734313964844\n",
      "Training: 3 epoch. 1500 iteration. Loss: 1.0094316005706787\n",
      "Training: 3 epoch. 1600 iteration. Loss: 0.4706525206565857\n",
      "Training: 3 epoch. 1700 iteration. Loss: 0.13498812913894653\n",
      "Training: 3 epoch. 1800 iteration. Loss: 0.8605109453201294\n",
      "Training: 3 epoch. 1900 iteration. Loss: 0.569390058517456\n",
      "Training: 3 epoch. 2000 iteration. Loss: 1.207332968711853\n",
      "Training: 3 epoch. 2100 iteration. Loss: 1.0373693704605103\n",
      "Training: 3 epoch. 2200 iteration. Loss: 1.1620815992355347\n",
      "Training: 3 epoch. 2300 iteration. Loss: 0.8937762975692749\n",
      "Training: 3 epoch. 2400 iteration. Loss: 0.6727941632270813\n",
      "Training: 3 epoch. 2500 iteration. Loss: 0.7953380942344666\n",
      "Training: 3 epoch. 2600 iteration. Loss: 0.5280734300613403\n",
      "Training: 3 epoch. 2700 iteration. Loss: 1.613365650177002\n",
      "Training: 3 epoch. 2800 iteration. Loss: 0.4507797360420227\n",
      "Training: 3 epoch. 2900 iteration. Loss: 1.8106173276901245\n",
      "Training: 3 epoch. 3000 iteration. Loss: 0.9937489628791809\n",
      "Training: 3 epoch. 3100 iteration. Loss: 0.11108437180519104\n",
      "Training: 3 epoch. 3200 iteration. Loss: 0.4428044557571411\n",
      "Training: 3 epoch. 3300 iteration. Loss: 1.1182643175125122\n",
      "Training: 3 epoch. 3400 iteration. Loss: 0.11050476133823395\n",
      "Training: 3 epoch. 3500 iteration. Loss: 0.7721740007400513\n",
      "Training: 3 epoch. 3600 iteration. Loss: 0.6116305589675903\n",
      "Training: 3 epoch. 3700 iteration. Loss: 0.670882523059845\n",
      "Training: 3 epoch. 3800 iteration. Loss: 0.8227506279945374\n",
      "Training: 3 epoch. 3900 iteration. Loss: 1.399368166923523\n",
      "Training: 3 epoch. 4000 iteration. Loss: 2.341883897781372\n",
      "Training: 3 epoch. 4100 iteration. Loss: 1.1464078426361084\n",
      "Training: 3 epoch. 4200 iteration. Loss: 1.4833714962005615\n",
      "Training: 3 epoch. 4300 iteration. Loss: 1.8062636852264404\n",
      "Training: 3 epoch. 4400 iteration. Loss: 0.4816320836544037\n",
      "Training: 3 epoch. 4500 iteration. Loss: 0.3332662582397461\n",
      "Training: 3 epoch. 4600 iteration. Loss: 1.2157912254333496\n",
      "Training: 3 epoch. 4700 iteration. Loss: 0.5032083988189697\n",
      "Training: 3 epoch. 4800 iteration. Loss: 1.0248318910598755\n",
      "Training: 3 epoch. 4900 iteration. Loss: 0.13639934360980988\n",
      "Training: 3 epoch. 5000 iteration. Loss: 1.6572456359863281\n",
      "Training: 3 epoch. 5100 iteration. Loss: 1.2736504077911377\n",
      "Training: 3 epoch. 5200 iteration. Loss: 0.9130617380142212\n",
      "Training: 3 epoch. 5300 iteration. Loss: 0.7031712532043457\n",
      "Training: 3 epoch. 5400 iteration. Loss: 0.7900993824005127\n",
      "Training: 3 epoch. 5500 iteration. Loss: 0.4378039538860321\n",
      "Training: 3 epoch. 5600 iteration. Loss: 1.043699026107788\n",
      "Training: 3 epoch. 5700 iteration. Loss: 0.45074522495269775\n",
      "Training: 3 epoch. 5800 iteration. Loss: 0.07861991226673126\n",
      "Training: 3 epoch. 5900 iteration. Loss: 0.5742189884185791\n",
      "Training: 3 epoch. 6000 iteration. Loss: 0.45616188645362854\n",
      "Training: 3 epoch. 6100 iteration. Loss: 0.5592695474624634\n",
      "Training: 3 epoch. 6200 iteration. Loss: 0.7577078342437744\n",
      "Training: 3 epoch. 6300 iteration. Loss: 0.644626796245575\n",
      "Training: 3 epoch. 6400 iteration. Loss: 1.8398405313491821\n",
      "Training: 3 epoch. 6500 iteration. Loss: 1.628967523574829\n",
      "Training: 3 epoch. 6600 iteration. Loss: 0.659214437007904\n",
      "Training: 3 epoch. 6700 iteration. Loss: 0.31668755412101746\n",
      "Training: 3 epoch. 6800 iteration. Loss: 1.9470256567001343\n",
      "Training: 3 epoch. 6900 iteration. Loss: 0.6380786299705505\n",
      "Training: 3 epoch. 7000 iteration. Loss: 1.1722017526626587\n",
      "Training: 3 epoch. 7100 iteration. Loss: 1.1117045879364014\n",
      "Training: 3 epoch. 7200 iteration. Loss: 1.0481003522872925\n",
      "Training: 3 epoch. 7300 iteration. Loss: 1.407135009765625\n",
      "Training: 3 epoch. 7400 iteration. Loss: 0.7463735938072205\n",
      "Training: 3 epoch. 7500 iteration. Loss: 0.5800826549530029\n",
      "Training: 3 epoch. 7600 iteration. Loss: 1.0291786193847656\n",
      "Training: 3 epoch. 7700 iteration. Loss: 0.923081636428833\n",
      "Training: 3 epoch. 7800 iteration. Loss: 0.5161335468292236\n",
      "Training: 3 epoch. 7900 iteration. Loss: 0.26744481921195984\n",
      "Training: 3 epoch. 8000 iteration. Loss: 0.6422594785690308\n",
      "Training: 3 epoch. 8100 iteration. Loss: 0.9104430675506592\n",
      "Training: 3 epoch. 8200 iteration. Loss: 1.0190083980560303\n",
      "Training: 3 epoch. 8300 iteration. Loss: 0.660838782787323\n",
      "Training: 3 epoch. 8400 iteration. Loss: 1.5403586626052856\n",
      "Training: 3 epoch. 8500 iteration. Loss: 1.157786250114441\n",
      "Training: 3 epoch. 8600 iteration. Loss: 1.5038706064224243\n",
      "Training: 3 epoch. 8700 iteration. Loss: 0.813665509223938\n",
      "Training: 3 epoch. 8800 iteration. Loss: 0.5953094959259033\n",
      "Training: 3 epoch. 8900 iteration. Loss: 1.8279139995574951\n",
      "Training: 3 epoch. 9000 iteration. Loss: 0.33290937542915344\n",
      "Training: 3 epoch. 9100 iteration. Loss: 1.1191649436950684\n",
      "Training: 3 epoch. 9200 iteration. Loss: 0.9701106548309326\n",
      "Training: 3 epoch. 9300 iteration. Loss: 0.7825249433517456\n",
      "Training: 3 epoch. 9400 iteration. Loss: 1.495150089263916\n",
      "Training: 3 epoch. 9500 iteration. Loss: 0.6854103803634644\n",
      "Training: 3 epoch. 9600 iteration. Loss: 1.2407721281051636\n",
      "Training: 3 epoch. 9700 iteration. Loss: 0.45037415623664856\n",
      "Training: 3 epoch. 9800 iteration. Loss: 1.1649572849273682\n",
      "Training: 3 epoch. 9900 iteration. Loss: 0.12224531173706055\n",
      "Training: 3 epoch. 10000 iteration. Loss: 0.3894168436527252\n",
      "Training: 3 epoch. 10100 iteration. Loss: 0.6528685092926025\n",
      "Training: 3 epoch. 10200 iteration. Loss: 0.9317042827606201\n",
      "Training: 3 epoch. 10300 iteration. Loss: 0.4266252815723419\n",
      "Training: 3 epoch. 10400 iteration. Loss: 0.5342531800270081\n",
      "Training: 3 epoch. 10500 iteration. Loss: 0.24502773582935333\n",
      "Training: 3 epoch. 10600 iteration. Loss: 0.4204985499382019\n",
      "Training: 3 epoch. 10700 iteration. Loss: 0.34149259328842163\n",
      "Training: 3 epoch. 10800 iteration. Loss: 1.074439525604248\n",
      "Training: 3 epoch. 10900 iteration. Loss: 0.27917104959487915\n",
      "Training: 3 epoch. 11000 iteration. Loss: 0.9198940992355347\n",
      "Training: 3 epoch. 11100 iteration. Loss: 1.4620838165283203\n",
      "Training: 3 epoch. 11200 iteration. Loss: 0.14638042449951172\n",
      "Training: 3 epoch. 11300 iteration. Loss: 0.4508220851421356\n",
      "Training: 3 epoch. 11400 iteration. Loss: 0.6324824690818787\n",
      "Training: 3 epoch. 11500 iteration. Loss: 0.7440891265869141\n",
      "Training: 3 epoch. 11600 iteration. Loss: 0.2492477297782898\n",
      "Training: 3 epoch. 11700 iteration. Loss: 0.7862148284912109\n",
      "Training: 3 epoch. 11800 iteration. Loss: 0.6302056312561035\n",
      "Training: 3 epoch. 11900 iteration. Loss: 0.9540123343467712\n",
      "Training: 3 epoch. 12000 iteration. Loss: 0.7606300711631775\n",
      "Training: 3 epoch. 12100 iteration. Loss: 0.8205382823944092\n",
      "Training: 3 epoch. 12200 iteration. Loss: 1.2197794914245605\n",
      "Training: 3 epoch. 12300 iteration. Loss: 0.3049885630607605\n",
      "Training: 3 epoch. 12400 iteration. Loss: 0.5126698017120361\n",
      "Training: 3 epoch. 12500 iteration. Loss: 0.1484716534614563\n",
      "Training loss (ave.): 0.8362931236011348\n",
      "\n",
      "Validation start\n",
      "Validation loss: 3.5588328415073454, Accuracy: 0.6935\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 4 epoch. 100 iteration. Loss: 1.1868013143539429\n",
      "Training: 4 epoch. 200 iteration. Loss: 0.8195555210113525\n",
      "Training: 4 epoch. 300 iteration. Loss: 0.5952836871147156\n",
      "Training: 4 epoch. 400 iteration. Loss: 0.27135124802589417\n",
      "Training: 4 epoch. 500 iteration. Loss: 0.984259843826294\n",
      "Training: 4 epoch. 600 iteration. Loss: 0.06389867514371872\n",
      "Training: 4 epoch. 700 iteration. Loss: 0.3137345016002655\n",
      "Training: 4 epoch. 800 iteration. Loss: 1.242606282234192\n",
      "Training: 4 epoch. 900 iteration. Loss: 1.4986647367477417\n",
      "Training: 4 epoch. 1000 iteration. Loss: 0.30125105381011963\n",
      "Training: 4 epoch. 1100 iteration. Loss: 0.8221474289894104\n",
      "Training: 4 epoch. 1200 iteration. Loss: 1.0072928667068481\n",
      "Training: 4 epoch. 1300 iteration. Loss: 0.03285648673772812\n",
      "Training: 4 epoch. 1400 iteration. Loss: 1.0570114850997925\n",
      "Training: 4 epoch. 1500 iteration. Loss: 1.2897461652755737\n",
      "Training: 4 epoch. 1600 iteration. Loss: 0.9318902492523193\n",
      "Training: 4 epoch. 1700 iteration. Loss: 0.26642608642578125\n",
      "Training: 4 epoch. 1800 iteration. Loss: 2.4455888271331787\n",
      "Training: 4 epoch. 1900 iteration. Loss: 1.1348183155059814\n",
      "Training: 4 epoch. 2000 iteration. Loss: 0.41542789340019226\n",
      "Training: 4 epoch. 2100 iteration. Loss: 2.711191415786743\n",
      "Training: 4 epoch. 2200 iteration. Loss: 0.7110204696655273\n",
      "Training: 4 epoch. 2300 iteration. Loss: 0.3004452884197235\n",
      "Training: 4 epoch. 2400 iteration. Loss: 0.39384743571281433\n",
      "Training: 4 epoch. 2500 iteration. Loss: 0.1476675570011139\n",
      "Training: 4 epoch. 2600 iteration. Loss: 0.7845011949539185\n",
      "Training: 4 epoch. 2700 iteration. Loss: 0.2126970738172531\n",
      "Training: 4 epoch. 2800 iteration. Loss: 0.628533124923706\n",
      "Training: 4 epoch. 2900 iteration. Loss: 0.6225273013114929\n",
      "Training: 4 epoch. 3000 iteration. Loss: 0.6506758332252502\n",
      "Training: 4 epoch. 3100 iteration. Loss: 0.3730354905128479\n",
      "Training: 4 epoch. 3200 iteration. Loss: 0.6597919464111328\n",
      "Training: 4 epoch. 3300 iteration. Loss: 0.05619075149297714\n",
      "Training: 4 epoch. 3400 iteration. Loss: 0.36332249641418457\n",
      "Training: 4 epoch. 3500 iteration. Loss: 0.9565298557281494\n",
      "Training: 4 epoch. 3600 iteration. Loss: 0.8396542072296143\n",
      "Training: 4 epoch. 3700 iteration. Loss: 0.39723458886146545\n",
      "Training: 4 epoch. 3800 iteration. Loss: 1.5036232471466064\n",
      "Training: 4 epoch. 3900 iteration. Loss: 0.10964635014533997\n",
      "Training: 4 epoch. 4000 iteration. Loss: 1.9143292903900146\n",
      "Training: 4 epoch. 4100 iteration. Loss: 0.44098877906799316\n",
      "Training: 4 epoch. 4200 iteration. Loss: 1.5908609628677368\n",
      "Training: 4 epoch. 4300 iteration. Loss: 2.5694892406463623\n",
      "Training: 4 epoch. 4400 iteration. Loss: 0.44900500774383545\n",
      "Training: 4 epoch. 4500 iteration. Loss: 0.5155830383300781\n",
      "Training: 4 epoch. 4600 iteration. Loss: 0.4769895672798157\n",
      "Training: 4 epoch. 4700 iteration. Loss: 1.2511781454086304\n",
      "Training: 4 epoch. 4800 iteration. Loss: 0.29936596751213074\n",
      "Training: 4 epoch. 4900 iteration. Loss: 0.5088852047920227\n",
      "Training: 4 epoch. 5000 iteration. Loss: 0.27928829193115234\n",
      "Training: 4 epoch. 5100 iteration. Loss: 0.6691538095474243\n",
      "Training: 4 epoch. 5200 iteration. Loss: 0.9670220017433167\n",
      "Training: 4 epoch. 5300 iteration. Loss: 0.3847680985927582\n",
      "Training: 4 epoch. 5400 iteration. Loss: 1.3485567569732666\n",
      "Training: 4 epoch. 5500 iteration. Loss: 1.8517382144927979\n",
      "Training: 4 epoch. 5600 iteration. Loss: 1.5758440494537354\n",
      "Training: 4 epoch. 5700 iteration. Loss: 1.1685361862182617\n",
      "Training: 4 epoch. 5800 iteration. Loss: 0.7655368447303772\n",
      "Training: 4 epoch. 5900 iteration. Loss: 0.6503214836120605\n",
      "Training: 4 epoch. 6000 iteration. Loss: 0.968670129776001\n",
      "Training: 4 epoch. 6100 iteration. Loss: 0.5311270952224731\n",
      "Training: 4 epoch. 6200 iteration. Loss: 0.5371605157852173\n",
      "Training: 4 epoch. 6300 iteration. Loss: 0.6309095621109009\n",
      "Training: 4 epoch. 6400 iteration. Loss: 1.3879598379135132\n",
      "Training: 4 epoch. 6500 iteration. Loss: 0.178562730550766\n",
      "Training: 4 epoch. 6600 iteration. Loss: 0.8924620151519775\n",
      "Training: 4 epoch. 6700 iteration. Loss: 1.153489112854004\n",
      "Training: 4 epoch. 6800 iteration. Loss: 0.9842835068702698\n",
      "Training: 4 epoch. 6900 iteration. Loss: 0.37157195806503296\n",
      "Training: 4 epoch. 7000 iteration. Loss: 0.09144040197134018\n",
      "Training: 4 epoch. 7100 iteration. Loss: 0.726308286190033\n",
      "Training: 4 epoch. 7200 iteration. Loss: 1.153928518295288\n",
      "Training: 4 epoch. 7300 iteration. Loss: 0.1357954442501068\n",
      "Training: 4 epoch. 7400 iteration. Loss: 1.291640043258667\n",
      "Training: 4 epoch. 7500 iteration. Loss: 0.12277600914239883\n",
      "Training: 4 epoch. 7600 iteration. Loss: 0.8999544978141785\n",
      "Training: 4 epoch. 7700 iteration. Loss: 1.6231117248535156\n",
      "Training: 4 epoch. 7800 iteration. Loss: 0.46703964471817017\n",
      "Training: 4 epoch. 7900 iteration. Loss: 0.4332304298877716\n",
      "Training: 4 epoch. 8000 iteration. Loss: 1.8664605617523193\n",
      "Training: 4 epoch. 8100 iteration. Loss: 1.2709158658981323\n",
      "Training: 4 epoch. 8200 iteration. Loss: 0.37352120876312256\n",
      "Training: 4 epoch. 8300 iteration. Loss: 0.44253382086753845\n",
      "Training: 4 epoch. 8400 iteration. Loss: 1.8974876403808594\n",
      "Training: 4 epoch. 8500 iteration. Loss: 1.8554563522338867\n",
      "Training: 4 epoch. 8600 iteration. Loss: 0.5470752716064453\n",
      "Training: 4 epoch. 8700 iteration. Loss: 0.9132217168807983\n",
      "Training: 4 epoch. 8800 iteration. Loss: 1.241314172744751\n",
      "Training: 4 epoch. 8900 iteration. Loss: 0.19587872922420502\n",
      "Training: 4 epoch. 9000 iteration. Loss: 0.7877404093742371\n",
      "Training: 4 epoch. 9100 iteration. Loss: 0.8940562605857849\n",
      "Training: 4 epoch. 9200 iteration. Loss: 0.3286854326725006\n",
      "Training: 4 epoch. 9300 iteration. Loss: 1.0942912101745605\n",
      "Training: 4 epoch. 9400 iteration. Loss: 0.5788383483886719\n",
      "Training: 4 epoch. 9500 iteration. Loss: 1.84237802028656\n",
      "Training: 4 epoch. 9600 iteration. Loss: 0.970391035079956\n",
      "Training: 4 epoch. 9700 iteration. Loss: 0.6317962408065796\n",
      "Training: 4 epoch. 9800 iteration. Loss: 0.4825943112373352\n",
      "Training: 4 epoch. 9900 iteration. Loss: 1.0543023347854614\n",
      "Training: 4 epoch. 10000 iteration. Loss: 0.11757709830999374\n",
      "Training: 4 epoch. 10100 iteration. Loss: 1.0430511236190796\n",
      "Training: 4 epoch. 10200 iteration. Loss: 0.658045768737793\n",
      "Training: 4 epoch. 10300 iteration. Loss: 0.43580520153045654\n",
      "Training: 4 epoch. 10400 iteration. Loss: 1.3262927532196045\n",
      "Training: 4 epoch. 10500 iteration. Loss: 0.05090145021677017\n",
      "Training: 4 epoch. 10600 iteration. Loss: 0.6724454164505005\n",
      "Training: 4 epoch. 10700 iteration. Loss: 0.6545417904853821\n",
      "Training: 4 epoch. 10800 iteration. Loss: 0.06341206282377243\n",
      "Training: 4 epoch. 10900 iteration. Loss: 1.3997128009796143\n",
      "Training: 4 epoch. 11000 iteration. Loss: 0.9206418991088867\n",
      "Training: 4 epoch. 11100 iteration. Loss: 0.50238436460495\n",
      "Training: 4 epoch. 11200 iteration. Loss: 0.10056184977293015\n",
      "Training: 4 epoch. 11300 iteration. Loss: 0.27141332626342773\n",
      "Training: 4 epoch. 11400 iteration. Loss: 1.0273633003234863\n",
      "Training: 4 epoch. 11500 iteration. Loss: 0.039837006479501724\n",
      "Training: 4 epoch. 11600 iteration. Loss: 0.3789409399032593\n",
      "Training: 4 epoch. 11700 iteration. Loss: 2.1714468002319336\n",
      "Training: 4 epoch. 11800 iteration. Loss: 1.0862916707992554\n",
      "Training: 4 epoch. 11900 iteration. Loss: 0.4674777686595917\n",
      "Training: 4 epoch. 12000 iteration. Loss: 1.8284932374954224\n",
      "Training: 4 epoch. 12100 iteration. Loss: 1.3487423658370972\n",
      "Training: 4 epoch. 12200 iteration. Loss: 0.5813149809837341\n",
      "Training: 4 epoch. 12300 iteration. Loss: 0.013476204127073288\n",
      "Training: 4 epoch. 12400 iteration. Loss: 1.3737205266952515\n",
      "Training: 4 epoch. 12500 iteration. Loss: 0.3659200668334961\n",
      "Training loss (ave.): 0.7247279247209336\n",
      "\n",
      "Validation start\n",
      "Validation loss: 3.630486036171764, Accuracy: 0.6912\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 5 epoch. 100 iteration. Loss: 1.6667965650558472\n",
      "Training: 5 epoch. 200 iteration. Loss: 0.9577000141143799\n",
      "Training: 5 epoch. 300 iteration. Loss: 0.35017716884613037\n",
      "Training: 5 epoch. 400 iteration. Loss: 0.08807025849819183\n",
      "Training: 5 epoch. 500 iteration. Loss: 0.1608394980430603\n",
      "Training: 5 epoch. 600 iteration. Loss: 0.3212520182132721\n",
      "Training: 5 epoch. 700 iteration. Loss: 0.3643937408924103\n",
      "Training: 5 epoch. 800 iteration. Loss: 1.0149354934692383\n",
      "Training: 5 epoch. 900 iteration. Loss: 0.3720870912075043\n",
      "Training: 5 epoch. 1000 iteration. Loss: 0.8067103028297424\n",
      "Training: 5 epoch. 1100 iteration. Loss: 0.3328828513622284\n",
      "Training: 5 epoch. 1200 iteration. Loss: 0.8291842341423035\n",
      "Training: 5 epoch. 1300 iteration. Loss: 0.5466110110282898\n",
      "Training: 5 epoch. 1400 iteration. Loss: 0.28809303045272827\n",
      "Training: 5 epoch. 1500 iteration. Loss: 1.9144002199172974\n",
      "Training: 5 epoch. 1600 iteration. Loss: 0.22363212704658508\n",
      "Training: 5 epoch. 1700 iteration. Loss: 0.5744841694831848\n",
      "Training: 5 epoch. 1800 iteration. Loss: 0.13598719239234924\n",
      "Training: 5 epoch. 1900 iteration. Loss: 0.2683353126049042\n",
      "Training: 5 epoch. 2000 iteration. Loss: 1.1568337678909302\n",
      "Training: 5 epoch. 2100 iteration. Loss: 0.7772133946418762\n",
      "Training: 5 epoch. 2200 iteration. Loss: 0.08651569485664368\n",
      "Training: 5 epoch. 2300 iteration. Loss: 0.16522225737571716\n",
      "Training: 5 epoch. 2400 iteration. Loss: 0.5621703863143921\n",
      "Training: 5 epoch. 2500 iteration. Loss: 0.42924365401268005\n",
      "Training: 5 epoch. 2600 iteration. Loss: 0.36706846952438354\n",
      "Training: 5 epoch. 2700 iteration. Loss: 0.04225051775574684\n",
      "Training: 5 epoch. 2800 iteration. Loss: 0.39050042629241943\n",
      "Training: 5 epoch. 2900 iteration. Loss: 1.391045331954956\n",
      "Training: 5 epoch. 3000 iteration. Loss: 0.768632173538208\n",
      "Training: 5 epoch. 3100 iteration. Loss: 0.33727794885635376\n",
      "Training: 5 epoch. 3200 iteration. Loss: 0.6743671894073486\n",
      "Training: 5 epoch. 3300 iteration. Loss: 1.3675117492675781\n",
      "Training: 5 epoch. 3400 iteration. Loss: 0.5985001921653748\n",
      "Training: 5 epoch. 3500 iteration. Loss: 1.3763238191604614\n",
      "Training: 5 epoch. 3600 iteration. Loss: 0.008422194048762321\n",
      "Training: 5 epoch. 3700 iteration. Loss: 0.4606245756149292\n",
      "Training: 5 epoch. 3800 iteration. Loss: 0.26918405294418335\n",
      "Training: 5 epoch. 3900 iteration. Loss: 1.8303184509277344\n",
      "Training: 5 epoch. 4000 iteration. Loss: 2.219125509262085\n",
      "Training: 5 epoch. 4100 iteration. Loss: 0.37883907556533813\n",
      "Training: 5 epoch. 4200 iteration. Loss: 2.4397053718566895\n",
      "Training: 5 epoch. 4300 iteration. Loss: 0.5212070345878601\n",
      "Training: 5 epoch. 4400 iteration. Loss: 0.13381198048591614\n",
      "Training: 5 epoch. 4500 iteration. Loss: 0.7634809613227844\n",
      "Training: 5 epoch. 4600 iteration. Loss: 0.28754666447639465\n",
      "Training: 5 epoch. 4700 iteration. Loss: 0.5557225942611694\n",
      "Training: 5 epoch. 4800 iteration. Loss: 1.4036545753479004\n",
      "Training: 5 epoch. 4900 iteration. Loss: 1.8677446842193604\n",
      "Training: 5 epoch. 5000 iteration. Loss: 0.5610838532447815\n",
      "Training: 5 epoch. 5100 iteration. Loss: 1.4873881340026855\n",
      "Training: 5 epoch. 5200 iteration. Loss: 0.5847574472427368\n",
      "Training: 5 epoch. 5300 iteration. Loss: 0.9043182134628296\n",
      "Training: 5 epoch. 5400 iteration. Loss: 0.32097434997558594\n",
      "Training: 5 epoch. 5500 iteration. Loss: 0.18587949872016907\n",
      "Training: 5 epoch. 5600 iteration. Loss: 0.15452885627746582\n",
      "Training: 5 epoch. 5700 iteration. Loss: 0.47534000873565674\n",
      "Training: 5 epoch. 5800 iteration. Loss: 0.22722648084163666\n",
      "Training: 5 epoch. 5900 iteration. Loss: 0.3251928985118866\n",
      "Training: 5 epoch. 6000 iteration. Loss: 0.19224299490451813\n",
      "Training: 5 epoch. 6100 iteration. Loss: 0.6827980279922485\n",
      "Training: 5 epoch. 6200 iteration. Loss: 0.928808867931366\n",
      "Training: 5 epoch. 6300 iteration. Loss: 0.19615405797958374\n",
      "Training: 5 epoch. 6400 iteration. Loss: 0.995080828666687\n",
      "Training: 5 epoch. 6500 iteration. Loss: 0.5978147387504578\n",
      "Training: 5 epoch. 6600 iteration. Loss: 1.3246204853057861\n",
      "Training: 5 epoch. 6700 iteration. Loss: 0.8101030588150024\n",
      "Training: 5 epoch. 6800 iteration. Loss: 0.23177361488342285\n",
      "Training: 5 epoch. 6900 iteration. Loss: 0.782781720161438\n",
      "Training: 5 epoch. 7000 iteration. Loss: 0.2666037678718567\n",
      "Training: 5 epoch. 7100 iteration. Loss: 0.7794156670570374\n",
      "Training: 5 epoch. 7200 iteration. Loss: 0.44003063440322876\n",
      "Training: 5 epoch. 7300 iteration. Loss: 0.7595934271812439\n",
      "Training: 5 epoch. 7400 iteration. Loss: 0.18539510667324066\n",
      "Training: 5 epoch. 7500 iteration. Loss: 0.7955166101455688\n",
      "Training: 5 epoch. 7600 iteration. Loss: 0.20353969931602478\n",
      "Training: 5 epoch. 7700 iteration. Loss: 0.6084680557250977\n",
      "Training: 5 epoch. 7800 iteration. Loss: 0.5161768198013306\n",
      "Training: 5 epoch. 7900 iteration. Loss: 1.6529641151428223\n",
      "Training: 5 epoch. 8000 iteration. Loss: 0.9686381816864014\n",
      "Training: 5 epoch. 8100 iteration. Loss: 0.40021148324012756\n",
      "Training: 5 epoch. 8200 iteration. Loss: 1.3641945123672485\n",
      "Training: 5 epoch. 8300 iteration. Loss: 1.4466309547424316\n",
      "Training: 5 epoch. 8400 iteration. Loss: 0.07055801898241043\n",
      "Training: 5 epoch. 8500 iteration. Loss: 0.4015754461288452\n",
      "Training: 5 epoch. 8600 iteration. Loss: 0.08378497511148453\n",
      "Training: 5 epoch. 8700 iteration. Loss: 0.4548436999320984\n",
      "Training: 5 epoch. 8800 iteration. Loss: 0.26911091804504395\n",
      "Training: 5 epoch. 8900 iteration. Loss: 0.6048707365989685\n",
      "Training: 5 epoch. 9000 iteration. Loss: 0.9971909523010254\n",
      "Training: 5 epoch. 9100 iteration. Loss: 0.9421007037162781\n",
      "Training: 5 epoch. 9200 iteration. Loss: 1.3315081596374512\n",
      "Training: 5 epoch. 9300 iteration. Loss: 0.3920862674713135\n",
      "Training: 5 epoch. 9400 iteration. Loss: 0.6971005797386169\n",
      "Training: 5 epoch. 9500 iteration. Loss: 2.1050684452056885\n",
      "Training: 5 epoch. 9600 iteration. Loss: 1.082513451576233\n",
      "Training: 5 epoch. 9700 iteration. Loss: 0.4226105809211731\n",
      "Training: 5 epoch. 9800 iteration. Loss: 2.1736555099487305\n",
      "Training: 5 epoch. 9900 iteration. Loss: 0.8921847939491272\n",
      "Training: 5 epoch. 10000 iteration. Loss: 0.2430075705051422\n",
      "Training: 5 epoch. 10100 iteration. Loss: 1.04398775100708\n",
      "Training: 5 epoch. 10200 iteration. Loss: 0.5091837644577026\n",
      "Training: 5 epoch. 10300 iteration. Loss: 0.3375946581363678\n",
      "Training: 5 epoch. 10400 iteration. Loss: 0.26297271251678467\n",
      "Training: 5 epoch. 10500 iteration. Loss: 0.3916263282299042\n",
      "Training: 5 epoch. 10600 iteration. Loss: 1.2219245433807373\n",
      "Training: 5 epoch. 10700 iteration. Loss: 0.3615958094596863\n",
      "Training: 5 epoch. 10800 iteration. Loss: 1.0484901666641235\n",
      "Training: 5 epoch. 10900 iteration. Loss: 0.3675829768180847\n",
      "Training: 5 epoch. 11000 iteration. Loss: 0.028968123719096184\n",
      "Training: 5 epoch. 11100 iteration. Loss: 0.1405210793018341\n",
      "Training: 5 epoch. 11200 iteration. Loss: 0.6907907724380493\n",
      "Training: 5 epoch. 11300 iteration. Loss: 0.6399239301681519\n",
      "Training: 5 epoch. 11400 iteration. Loss: 1.0244542360305786\n",
      "Training: 5 epoch. 11500 iteration. Loss: 0.4155830442905426\n",
      "Training: 5 epoch. 11600 iteration. Loss: 1.1008716821670532\n",
      "Training: 5 epoch. 11700 iteration. Loss: 0.29177626967430115\n",
      "Training: 5 epoch. 11800 iteration. Loss: 0.9707766771316528\n",
      "Training: 5 epoch. 11900 iteration. Loss: 0.24128428101539612\n",
      "Training: 5 epoch. 12000 iteration. Loss: 0.4799720048904419\n",
      "Training: 5 epoch. 12100 iteration. Loss: 0.43368250131607056\n",
      "Training: 5 epoch. 12200 iteration. Loss: 0.7643477916717529\n",
      "Training: 5 epoch. 12300 iteration. Loss: 0.4509800672531128\n",
      "Training: 5 epoch. 12400 iteration. Loss: 0.21321552991867065\n",
      "Training: 5 epoch. 12500 iteration. Loss: 0.1725224405527115\n",
      "Training loss (ave.): 0.6383922541268263\n",
      "\n",
      "Validation start\n",
      "Validation loss: 3.397442806380242, Accuracy: 0.7203\n",
      "\n",
      "step\n",
      "\n",
      "Train start\n",
      "Training: 6 epoch. 100 iteration. Loss: 0.40072640776634216\n",
      "Training: 6 epoch. 200 iteration. Loss: 0.15560223162174225\n",
      "Training: 6 epoch. 300 iteration. Loss: 0.30039408802986145\n",
      "Training: 6 epoch. 400 iteration. Loss: 0.30805379152297974\n",
      "Training: 6 epoch. 500 iteration. Loss: 0.0076285069808363914\n",
      "Training: 6 epoch. 600 iteration. Loss: 0.019447237253189087\n",
      "Training: 6 epoch. 700 iteration. Loss: 0.7664101123809814\n",
      "Training: 6 epoch. 800 iteration. Loss: 0.42949172854423523\n",
      "Training: 6 epoch. 900 iteration. Loss: 0.5202387571334839\n",
      "Training: 6 epoch. 1000 iteration. Loss: 0.38797131180763245\n",
      "Training: 6 epoch. 1100 iteration. Loss: 0.059216007590293884\n",
      "Training: 6 epoch. 1200 iteration. Loss: 0.1788424849510193\n",
      "Training: 6 epoch. 1300 iteration. Loss: 0.3821612298488617\n",
      "Training: 6 epoch. 1400 iteration. Loss: 0.6694406270980835\n",
      "Training: 6 epoch. 1500 iteration. Loss: 0.19898559153079987\n",
      "Training: 6 epoch. 1600 iteration. Loss: 1.8990461826324463\n",
      "Training: 6 epoch. 1700 iteration. Loss: 0.4789242148399353\n",
      "Training: 6 epoch. 1800 iteration. Loss: 1.1409701108932495\n",
      "Training: 6 epoch. 1900 iteration. Loss: 0.12942007184028625\n",
      "Training: 6 epoch. 2000 iteration. Loss: 1.815476894378662\n",
      "Training: 6 epoch. 2100 iteration. Loss: 0.7107000350952148\n",
      "Training: 6 epoch. 2200 iteration. Loss: 0.020175505429506302\n",
      "Training: 6 epoch. 2300 iteration. Loss: 0.734340488910675\n",
      "Training: 6 epoch. 2400 iteration. Loss: 0.5465518236160278\n",
      "Training: 6 epoch. 2500 iteration. Loss: 1.4498140811920166\n",
      "Training: 6 epoch. 2600 iteration. Loss: 0.5865854024887085\n",
      "Training: 6 epoch. 2700 iteration. Loss: 0.8626001477241516\n",
      "Training: 6 epoch. 2800 iteration. Loss: 0.6834880113601685\n",
      "Training: 6 epoch. 2900 iteration. Loss: 0.05492441728711128\n",
      "Training: 6 epoch. 3000 iteration. Loss: 0.2334102988243103\n",
      "Training: 6 epoch. 3100 iteration. Loss: 0.2208288013935089\n",
      "Training: 6 epoch. 3200 iteration. Loss: 0.12799575924873352\n",
      "Training: 6 epoch. 3300 iteration. Loss: 0.14288800954818726\n",
      "Training: 6 epoch. 3400 iteration. Loss: 0.22956156730651855\n",
      "Training: 6 epoch. 3500 iteration. Loss: 0.04952118545770645\n",
      "Training: 6 epoch. 3600 iteration. Loss: 0.4711245894432068\n",
      "Training: 6 epoch. 3700 iteration. Loss: 0.0018024659948423505\n",
      "Training: 6 epoch. 3800 iteration. Loss: 0.06287819892168045\n",
      "Training: 6 epoch. 3900 iteration. Loss: 0.9454665184020996\n",
      "Training: 6 epoch. 4000 iteration. Loss: 1.083531379699707\n",
      "Training: 6 epoch. 4100 iteration. Loss: 0.8773624897003174\n",
      "Training: 6 epoch. 4200 iteration. Loss: 0.27283790707588196\n",
      "Training: 6 epoch. 4300 iteration. Loss: 0.021254777908325195\n",
      "Training: 6 epoch. 4400 iteration. Loss: 0.23173081874847412\n",
      "Training: 6 epoch. 4500 iteration. Loss: 0.5498899221420288\n",
      "Training: 6 epoch. 4600 iteration. Loss: 0.8331918716430664\n",
      "Training: 6 epoch. 4700 iteration. Loss: 0.4430302083492279\n",
      "Training: 6 epoch. 4800 iteration. Loss: 1.127124309539795\n",
      "Training: 6 epoch. 4900 iteration. Loss: 0.24146302044391632\n",
      "Training: 6 epoch. 5000 iteration. Loss: 0.2764372229576111\n",
      "Training: 6 epoch. 5100 iteration. Loss: 0.24533405900001526\n",
      "Training: 6 epoch. 5200 iteration. Loss: 0.20704123377799988\n",
      "Training: 6 epoch. 5300 iteration. Loss: 0.80068039894104\n",
      "Training: 6 epoch. 5400 iteration. Loss: 0.44893261790275574\n",
      "Training: 6 epoch. 5500 iteration. Loss: 1.1359888315200806\n",
      "Training: 6 epoch. 5600 iteration. Loss: 1.298606514930725\n",
      "Training: 6 epoch. 5700 iteration. Loss: 0.11330869793891907\n",
      "Training: 6 epoch. 5800 iteration. Loss: 0.806041955947876\n",
      "Training: 6 epoch. 5900 iteration. Loss: 0.16544081270694733\n",
      "Training: 6 epoch. 6000 iteration. Loss: 1.085745930671692\n",
      "Training: 6 epoch. 6100 iteration. Loss: 2.079943895339966\n",
      "Training: 6 epoch. 6200 iteration. Loss: 0.0913030281662941\n",
      "Training: 6 epoch. 6300 iteration. Loss: 0.20139268040657043\n",
      "Training: 6 epoch. 6400 iteration. Loss: 0.16726304590702057\n",
      "Training: 6 epoch. 6500 iteration. Loss: 0.7615296244621277\n",
      "Training: 6 epoch. 6600 iteration. Loss: 0.2894282639026642\n",
      "Training: 6 epoch. 6700 iteration. Loss: 0.4485311508178711\n",
      "Training: 6 epoch. 6800 iteration. Loss: 0.9440971612930298\n",
      "Training: 6 epoch. 6900 iteration. Loss: 0.08975796401500702\n"
     ]
    }
   ],
   "source": [
    "run ../src/CIFAR10/CIFAR_train --epoch 30 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdd6dc-598c-4564-a185-c7a5d54f5b11",
   "metadata": {},
   "source": [
    "# Optunaによる自動パラメータ調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b42c0e-030b-427f-92e5-0a2bd1ce43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run ../src/CIFAR10/Optuna.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
